{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image alt](https://github.com/zoLikeCode/system_hack_mts/raw/main/mtstruetech.png)\n",
    "\n",
    "---\n",
    "### Содержание / навигация:\n",
    "1. [Описание](#описание)  \n",
    "2. [Установка и запуск проекта](#установка-и-запуск-проекта)  \n",
    "3. [Основной функционал проекта](#основной-функционал-проекта)  \n",
    "4. [Технологии и инструменты](#технологии-и-инструменты)  \n",
    "5. [Команда проекта](#команда-проекта)  \n",
    "6. [Архитектура проекта](#архитектура-и-структура-проекта)  \n",
    "7. [Демонстрация работы проекта](#демонстрация-работы-проекта)  \n",
    "8. [Заключение](#заключение)  \n",
    "\n",
    "---\n",
    "\n",
    "### Описание:\n",
    "Наша команда разработала проект, который помогает создать инклюзивную среду в многоквартирных домах с использованием технологий умного дома. Мы ориентируемся на потребности маломобильных и слабослышащих людей, предлагая инновационные решения для улучшения их повседневной жизни и повышения безопасности. Наше приложение позволяет автоматизировать ключевые элементы инфраструктуры — пандусы, двери, экстренные оповещения — улучшает взаимодействие с домофонами и облегчает навигацию для людей с ограниченными возможностями. Это помогает создать более удобные и безопасные условия для всех жильцов.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Установка и запуск проекта\n",
    "\n",
    "**Back:**\n",
    "1. Перейдите на сайт [https://console.mistral.ai](https://console.mistral.ai) и получите свой API ключ.\n",
    "2. В папке [`ML/NLP/`](ML/NLP/) создайте файл `.env` на основе примера [`.env.example`](ML/NLP/.env.example)\n",
    "3. После настройки `.env` файла выполните команду для сборки и запуска проекта:\n",
    "\n",
    "   ```bash\n",
    "   docker-compose up --build\n",
    "---\n",
    "\n",
    "### Основной функционал проекта:\n",
    "\n",
    "1. ИИ-ассистент, заточенный под ЖК.\n",
    "2. Bluetooth-метки для автоматизации инклюзивных элементов\n",
    "3. Автоматические оповещения о чрезвычайных ситуациях\n",
    "4. Расшифровка речи для граждан с нарушениями слуха\n",
    "5. Функция \"Кто дома?\"\n",
    "6. Персонализированнный сценарий для членов семьи\n",
    "\n",
    "---\n",
    "\n",
    "### Технологии и инструменты:\n",
    "\n",
    "1. ml: Vosk(Speech2Text), LangChain(LLM).\n",
    "2. backend: LangServe, FastAPI, unicorn.\n",
    "3. mobile: React Native, Expo Router, Expo SDK, Expo GO, Jotai, AsyncStorage.\n",
    "\n",
    "--- \n",
    "\n",
    "### Команда проекта:\n",
    "\n",
    "1. Светлана Шубина - продакт-менеджер, дизайнер.\n",
    "2. [Никита Зонтов](https://github.com/zoLikeCode) - backend-разработчик.\n",
    "3. [Денис Басанский](https://github.com/Bigilittle) - ml-специалист.\n",
    "4. [Павел Шабуров](https://github.com/Shavelo) - ml-специалист.\n",
    "5. [Максим Меркулов](https://github.com/spioncino) - mobile-разработчик.\n",
    "\n",
    "---\n",
    "\n",
    "### Архитектура проекта:\n",
    "\n",
    "---\n",
    "\n",
    "### Демонстрация работы проекта:\n",
    "\n",
    "1. [Видео]()\n",
    "2. [Презентация]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"aj3ozp7g0tZaWyvbmP8pHPrNGRgfJN7d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(api_key = API_KEY, model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Привет! У меня всё хорошо, спасибо. А у тебя как дела?', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 10, 'total_tokens': 38, 'completion_tokens': 28}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-9019417b-7888-42e9-8826-04615caf76dd-0', usage_metadata={'input_tokens': 10, 'output_tokens': 28, 'total_tokens': 38})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('привет, как дела?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import Field\n",
    "from langchain.chains import LLMChain\n",
    "import time\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "\n",
    "people_status = {\n",
    "    \"1001\": {\"status\": \"свободен\"},\n",
    "    \"1002\": {\"status\": \"занят\"},\n",
    "    \"1003\": {\"status\": \"свободен\"},\n",
    "    \"1004\": {\"status\": \"занят\"},\n",
    "    \"1005\": {\"status\": \"свободен\"}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "@tool\n",
    "def get_order_status(employee_id: str = Field(description=\"Identifier of Employee\")) -> str:\n",
    "    \"\"\"give the employee status. use when you need get a status of employee\"\"\"\n",
    "    return people_status.get(employee_id, f\"Не существует сотрудника\")\n",
    "\n",
    "\n",
    "tools = [get_order_status]\n",
    "llm = ChatMistralAI(api_key = API_KEY, model=\"mistral-large-latest\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "message = [\n",
    "    (\"system\", \"Отвечай только на русском языке. Ты являешься **ДОМОФОНОМ** и всегда общаешься уважительно и очень коротко, и отвечаешь только на вопросы человека про себя, или тот функцианал что у тебя есть\"),\n",
    "    (\"system\", \"Человека зовут **{name}** и ему **{age}**\"),\n",
    "    (\"human\", \"{message}\")\n",
    "]\n",
    "promt = ChatPromptTemplate(message)\n",
    "\n",
    "\n",
    "chain = promt | llm_with_tools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chatbot(state: State) -> State:\n",
    "    \"\"\"Обрабатывает сообщения и обновляет состояние.\"\"\"\n",
    "    time.sleep(1)\n",
    "\n",
    "    res = chain.invoke({\n",
    "        \"message\": state['messages'],\n",
    "        \"name\": state['name'],\n",
    "        \"age\": state['age']\n",
    "    })\n",
    "\n",
    "    if \"tool_calls\" in res.additional_kwargs:\n",
    "        return {\n",
    "            \"messages\": res,\n",
    "            \"name\": state['name'],\n",
    "            \"age\": state['age']\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"messages\": {\"role\": \"ai\", \"content\": res.content},\n",
    "        \"name\": state['name'],\n",
    "        \"age\": state['age']\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[get_order_status])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import Field, BaseModel\n",
    "import time\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv('../.env')\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def read_json_file(file_path: str) -> dict:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json_file(file_path: str, data: dict) -> None:\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "status_pass = read_json_file(\"./json/status_pass.json\")\n",
    "plumbers_requests = read_json_file('./json/plumbers_requests.json')\n",
    "\n",
    "class Employee(BaseModel):\n",
    "    full_name: str = Field(description=\"full name of the person who needs to be issued a pass\")\n",
    "    tenant_name: str = Field(description=\"The name of the person who orders the service\")\n",
    "\n",
    "class ApartmentRequest(BaseModel):\n",
    "    apartment_number: str = Field(description=\"the apartment number where the service is ordered\")\n",
    "    resident_name: str = Field(description=\"name of the customer of the service\")\n",
    "    time_of_visit: str = Field(description=\"time to visit\")\n",
    "\n",
    "\n",
    "\n",
    "@tool(args_schema=Employee)\n",
    "def add_employee_pass(full_name: str, tenant_name: str) -> str:\n",
    "    \"\"\"It is used when it is necessary to provide a pass to a person, it is recorded in the name of the customer.\"\"\"\n",
    "    employee_id = full_name.lower().replace(\" \", \"_\")  \n",
    "    status_pass[employee_id] = tenant_name\n",
    "\n",
    "    write_json_file(\"./json/status_pass.json\", status_pass)    \n",
    "    return f\"Пропуск для {full_name} успешно добавлен и истечёт через час\"\n",
    "\n",
    "@tool(args_schema=ApartmentRequest)\n",
    "def call_plumber(apartment_number: str, resident_name: str, time_of_visit: str) -> str:\n",
    "    \"\"\"Adds a request to call a plumber, including information about the apartment and the time of the visit.\"\"\"\n",
    "    request_id = f\"{apartment_number}_{resident_name.lower().replace(' ', '_')}\"\n",
    "\n",
    "    plumbers_requests[request_id] = {\n",
    "        \"apartment_number\":apartment_number,\n",
    "        \"resident_name\":resident_name,\n",
    "        \"time_of_visit\": time_of_visit\n",
    "    }\n",
    "    \n",
    "\n",
    "    write_json_file('./json/plumbers_requests.json', plumbers_requests)\n",
    "    return f\"Запрос на вызов сантехника для квартиры {apartment_number} принят. Время визита: {time_of_visit}\"\n",
    "\n",
    "\n",
    "\n",
    "tools = [add_employee_pass, call_plumber]\n",
    "llm = ChatMistralAI(api_key = API_KEY, model=\"mistral-large-latest\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "message = [\n",
    "    (\"system\", \"Отвечай только на русском языке. Ты являешься **ДОМОФОНОМ** и всегда общаешься уважительно и очень коротко, и отвечаешь только на вопросы человека про себя, или тот функцианал что у тебя есть\"),\n",
    "    (\"system\", \"Человека зовут **{name}** и ему **{age}**\"),\n",
    "    (\"human\", \"{message}\")\n",
    "]\n",
    "promt = ChatPromptTemplate(message)\n",
    "\n",
    "\n",
    "chain = promt | llm_with_tools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chatbot(state: State) -> State:\n",
    "    \"\"\"Обрабатывает сообщения и обновляет состояние.\"\"\"\n",
    "    time.sleep(1)\n",
    "\n",
    "    res = chain.invoke({\n",
    "        \"message\": state['messages'],\n",
    "        \"name\": state['name'],\n",
    "        \"age\": state['age']\n",
    "    })\n",
    "\n",
    "    if \"tool_calls\" in res.additional_kwargs:\n",
    "        return {\n",
    "            \"messages\": res,\n",
    "            \"name\": state['name'],\n",
    "            \"age\": state['age']\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"messages\": {\"role\": \"ai\", \"content\": res.content},\n",
    "        \"name\": state['name'],\n",
    "        \"age\": state['age']\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[add_employee_pass, call_plumber])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='закажи мне пожалуйста сантехника в квартиру 1024 на 18:00', additional_kwargs={}, response_metadata={}, id='6fcaaea6-281b-42b3-95da-049e66c893db'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'psuaBn09y', 'function': {'name': 'call_plumber', 'arguments': '{\"apartment_number\": \"1024\", \"resident_name\": \"Тестовый Тестович\", \"time_of_visit\": \"18:00\"}'}, 'index': 0}]}, response_metadata={'token_usage': {'prompt_tokens': 483, 'total_tokens': 540, 'completion_tokens': 57}, 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls'}, id='run-8f7073d4-805a-43f7-9450-d2e2d8d8ee9e-0', tool_calls=[{'name': 'call_plumber', 'args': {'apartment_number': '1024', 'resident_name': 'Тестовый Тестович', 'time_of_visit': '18:00'}, 'id': 'psuaBn09y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 483, 'output_tokens': 57, 'total_tokens': 540}),\n",
       "  ToolMessage(content='Запрос на вызов сантехника для квартиры 1024 принят. Время визита: 18:00', name='call_plumber', id='33b62584-be6e-4155-902f-29ef39cff6b1', tool_call_id='psuaBn09y'),\n",
       "  AIMessage(content='Запрос на вызов сантехника для квартиры 1024 принят. Время визита: 18:00', additional_kwargs={}, response_metadata={}, id='776c81d4-8076-436f-b968-69f98f71a9bb')],\n",
       " 'name': 'Тестовый Тестович',\n",
       " 'age': 20}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "user_input = \"закажи мне пожалуйста сантехника в квартиру 1024 на 18:00\"\n",
    "\n",
    "events = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
    "     \"name\": \"Тестовый Тестович\",\n",
    "     \"age\": 20\n",
    "     },\n",
    "    config\n",
    ")\n",
    "# for event in events:\n",
    "#     #.pretty_print()\n",
    "#     a = event[\"messages\"][-1]\n",
    "events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mкак у тебя дела?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m      5\u001b[0m     a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[1;32mc:\\Users\\bigil\\OneDrive\\Рабочий стол\\gitgotgot\\system_hack_mts\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:415\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bigil\\OneDrive\\Рабочий стол\\gitgotgot\\system_hack_mts\\.venv\\lib\\site-packages\\langchain_mistralai\\chat_models.py:597\u001b[0m, in \u001b[0;36mChatMistralAI._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    596\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 597\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    598\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    599\u001b[0m ):\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bigil\\OneDrive\\Рабочий стол\\gitgotgot\\system_hack_mts\\.venv\\lib\\site-packages\\langchain_mistralai\\chat_models.py:454\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry.<locals>.iter_sse\u001b[1;34m()\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miter_sse\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict]:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m connect_sse(\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    453\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event_source:\n\u001b[1;32m--> 454\u001b[0m         \u001b[43m_raise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_source\u001b[38;5;241m.\u001b[39miter_sse():\n\u001b[0;32m    456\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bigil\\OneDrive\\Рабочий стол\\gitgotgot\\system_hack_mts\\.venv\\lib\\site-packages\\langchain_mistralai\\chat_models.py:170\u001b[0m, in \u001b[0;36m_raise_on_error\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mis_error(response\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[0;32m    169\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError(\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError response \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    173\u001b[0m         request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m    174\u001b[0m         response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m    175\u001b[0m     )\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = ChatMistralAI(api_key = API_KEY, model=\"mistral-large-latest\")\n",
    "response = llm.stream(\"как у тебя дела?\")\n",
    "a = ''\n",
    "for value in response:\n",
    "    a += value.content\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
